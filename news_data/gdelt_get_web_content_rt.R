# 本程序实现今日俄罗斯爬虫，是gdelt_select_news_data.R的前续程序。
rm(list = ls())

library(stringr)
library(rvest)

data_address <- "gdelt_data/"
target_address <- "news/rt/"
gdelt_files <- list.files(data_address)
length_files <- length(gdelt_files)
title_length_limit <- 150

for (num in 1:length_files){
  if (str_detect(gdelt_files[num], ".csv$") == TRUE){
    print (str_sub(gdelt_files[num], 7, 14))
    csv_file <- read.csv(paste(data_address, gdelt_files[num], sep = ""))
    length_csv_file <- length(csv_file$MonthYear)
    print (length_csv_file)
    url_complete <- ''
    for (i in 1:length_csv_file){
      # 显示进度
      if (i %% 200 == 0){
        cat (str_sub(gdelt_files[num], 7, 14), ": ", round(i/length_csv_file, 2)*100, '%', '\n')
      }
      # other_website <- 1
      url <- csv_file[i, 'SOURCEURL'] %>% as.character()
      # url <- 'https://www.bbc.com/news/uk-northern-ireland-foyle-west-50328767'
      # 选出RT新闻并剔除明显无关内容
      if (is.na(url) == FALSE){  #确保url不为空
        conditions <- str_detect(url,'[.]rt.com') == TRUE && str_detect(url,'/tags/') == FALSE && str_detect(url,'/trends/') == FALSE && str_detect(url,'/on-air/') == FALSE && str_detect(url,'/podcast/') == FALSE
        if (conditions == TRUE){ # && str_detect(url,'/culture/') == FALSE && str_detect(url,'/programmes/') == FALSE && str_detect(url,'/cbbc/') == FALSE){
          if (str_detect(url_complete, url) == FALSE){           # 防止重复收集同一个网页的内容
            url_complete <- paste(url_complete, url, sep = ' ')
            print ('Find one!')
            print (i)
            # 避免网址不存在，出现404错误
            possibleError <- tryCatch(
              web <- read_html(url),
              error=function(e) e) #{
              # print('HTTP Error 404, go next.')})
            if(inherits(possibleError, "error")){
              cat ('HTTP Error 404, go next.\n')
              next
            }
            # web <- read_html(url)
            # 第一种网页，如https://apnews.com/article/72ef539bb781285d8b7efe8325796e5a：
            title <- web %>% html_nodes('h1.article__heading') %>% html_text()
            if (length(title) != 0){
              flag <- 1
              title <- title[1]     # 以免出现数行的title导致后续无法存储
              title <- substr(title, 1, title_length_limit) # 以免名称太长程序打不开txt文件
              title <- gsub("[^[:alnum:] ]", "", title)
              title <- gsub("^\\s+|\\s+$", "", title)
              # 命名规则：日期_原文件中的行数_网页类别_新闻标题.txt
              title <- paste(str_sub(gdelt_files[num], 7, 14), paste(as.character(i), paste(as.character(flag), title, sep = '_'), sep = '_'), sep = '_')
              text <- web %>% html_nodes('div.article__summary') %>% html_text()
              text2 <- web %>% html_nodes('div.article__text p') %>% html_text()
              if (length(text2) != 0){
                for (k in 1:length(text2)){text[k+1] <- text2[k]}
              }
            } else {
            # # 第二种网页，如https://apnews.com/article/97eca13a37dc39925694d439f649a348：
            #   title <- web %>% html_nodes('div.headline-0-2-56') %>% html_text()
            #   if (length(title) != 0){
            #     flag <- 2
            #     title <- title[1]
            #     title <- substr(title, 1, title_length_limit)
            #     title <- gsub("[^[:alnum:] ]", "", title)
            #     title <- paste(str_sub(gdelt_files[num], 7, 14), paste(as.character(i), paste(as.character(flag), title, sep = '_'), sep = '_'), sep = '_')
            #     text <- web %>% html_nodes('p.Component-root-0-2-73') %>% html_text()
            #   } else {
            # # 第三种网页，如https://www.bbc.com/pidgin/media-50985597：
            #     title <- web %>% html_nodes('div.gVouae h1') %>% html_text()
            #     if (length(title) != 0){
            #       flag <- 3
            #       title <- title[1]
            #       title <- substr(title, 1, title_length_limit)
            #       title <- gsub("[^[:alnum:] ]", "", title)
            #       title <- paste(str_sub(gdelt_files[num], 7, 14), paste(as.character(i), paste(as.character(flag), title, sep = '_'), sep = '_'), sep = '_')
            #       text <- web %>% html_nodes('div.fVauYi p') %>% html_text()
            #     } else {
            # # 第四种网页，如https://www.bbc.co.uk/newsround/51060735：
            #       title <- web %>% html_nodes('div.newsround-story-header__title-layout h1') %>% html_text()
            #       if (length(title) != 0){
            #         flag <- 4
            #         title <- title[1]
            #         title <- substr(title, 1, title_length_limit)
            #         title <- gsub("[^[:alnum:] ]", "", title)
            #         title <- paste(str_sub(gdelt_files[num], 7, 14), paste(as.character(i), paste(as.character(flag), title, sep = '_'), sep = '_'), sep = '_')
            #         text <- web %>% html_nodes('p.newsround-story-body__text') %>% html_text()
            #       } else {
            # # 第五种网页，如view-source:https://www.bbc.com/news/blogs-trending-51399306：
            #         title <- web %>% html_nodes('div.blog__story h2') %>% html_text()
            #         if (length(title) != 0){
            #           flag <- 5
            #           title <- title[1]
            #           title <- substr(title, 1, title_length_limit)
            #           title <- gsub("[^[:alnum:] ]", "", title)
            #           title <- paste(str_sub(gdelt_files[num], 7, 14), paste(as.character(i), paste(as.character(flag), title, sep = '_'), sep = '_'), sep = '_')
            #           text <- web %>% html_nodes('div.story-body__inner p') %>% html_text()
            #          } else {
            # 其它网页，记录下来
                     flag <- 0
                     title <- paste(str_sub(gdelt_files[num], 7, 14), paste(as.character(i), paste(as.character(flag), 'other_website', sep = '_'), sep = '_'), sep = '_')
                     text <- ''
                   }
            #      }
            #     }
            #   }
            # }
            # # title <- gsub('/', '', title)
            # # title <- gsub('£', '', title)
            # # title <- gsub("[^[:alnum:] ]", "", title)
            length_text <- length(text)
            text[length_text + 1] <- url
            
            write.table(text, file = paste(target_address, paste(title, '.txt', sep = ''), sep = ''), row.names = FALSE, col.names = FALSE)
            cat ('Web type: ', flag, '\n')
          }
        }
      }
    }
  }
}

cat ('Finished!')